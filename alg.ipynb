{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import math\n",
    "import heapq\n",
    "import queue\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.MultiDiGraph()\n",
    "random.seed(1)\n",
    "IS_DEBUG_MODE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Add nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add source nodes:\n",
    "\n",
    "- (Randomly) generate total supply ('total_supply') and unused supply ('unused_supply') for each source node;\n",
    "\n",
    "- Record all the total supply from all source nodes in 'grand_total_supply';\n",
    "\n",
    "- The 'demand' attribute is used for NetworkX's built-in min-cost flow algorithms. Negative demand means supply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_source = 3\n",
    "grand_total_supply = 0\n",
    "rand_supply = [0] * num_of_source\n",
    "for i in range(num_of_source):\n",
    "    rand_supply[i] = random.randint(1, 10)\n",
    "for i in range(num_of_source):\n",
    "    G.add_node('s' + str(i), demand = -rand_supply[i], total_supply = rand_supply[i], unused_supply = rand_supply[i])\n",
    "    grand_total_supply += rand_supply[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add sink nodes:\n",
    "\n",
    "- (Randomly) generate prior probabilities ('prior_prob'), and probabilies of detection ('detect_prob') for each sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_sink = 6\n",
    "rand_prior_prob = [0] * num_of_sink\n",
    "rand_detect_prob = [0] * num_of_sink\n",
    "for i in range(num_of_sink):\n",
    "    rand_prior_prob[i] = random.uniform(0.01, 0.99)\n",
    "sum_rand_prior_prob = sum(rand_prior_prob)\n",
    "rand_prior_prob = [a / sum_rand_prior_prob for a in rand_prior_prob]\n",
    "for i in range(num_of_sink):\n",
    "    rand_detect_prob[i] = random.uniform(0.01, 0.99)\n",
    "for i in range(num_of_sink):\n",
    "    G.add_node('t' + str(i), prior_prob = rand_prior_prob[i], detect_prob = rand_detect_prob[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add global sink node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.add_node('gt', demand = grand_total_supply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Add arcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Randomly) add arcs from sources to sinks:\n",
    "\n",
    "- Set flow = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_arc = 0  # Record number of arcs between sources and sinks \n",
    "connectivity_info = []\n",
    "\n",
    "for i in range(num_of_source):\n",
    "    for j in range(num_of_sink):\n",
    "        if random.random() > 0.5:\n",
    "            G.add_edge('s' + str(i), 't' + str(j), key = 0, flow = 0, weight = 0, capacity = G.node['s'+str(i)]['total_supply'])\n",
    "            num_of_arc = num_of_arc + 1\n",
    "            connectivity_info.append([i, j])\n",
    "\n",
    "# If a source is not connected to any sink, randomly connect it to some sink\n",
    "for i in range(num_of_source):\n",
    "    if not G.neighbors('s' + str(i)):\n",
    "        sink = random.randint(0, num_of_sink - 1)\n",
    "        G.add_edge('s' + str(i), 't' + str(sink), key = 0, flow = 0, weight = 0, capacity = G.node['s'+str(i)]['total_supply'])\n",
    "        num_of_arc = num_of_arc + 1\n",
    "        connectivity_info.append([i, sink])\n",
    "        \n",
    "# If a sink is not connected to any source, randomly connect it to some source\n",
    "for i in range(num_of_sink):\n",
    "    if not G.predecessors('t' + str(i)):\n",
    "        source = random.randint(0, num_of_source - 1)\n",
    "        G.add_edge('s' + str(source), 't' + str(i), key = 0, flow = 0, weight = 0, capacity = G.node['s'+str(source)]['total_supply'])\n",
    "        num_of_arc = num_of_arc + 1\n",
    "        connectivity_info.append([source, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add arcs from sinks to the global sink: \n",
    "\n",
    "- Set key = 1 (i.e., the 1st time to search this location); \n",
    "\n",
    "- Set flow = 0; \n",
    "\n",
    "- Set capacity = 1 (for NetworkX's built-in min-cost flow algorithms);\n",
    "\n",
    "- Compute and set weight (note: the negative of the probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(num_of_sink):\n",
    "    sink = 't'+str(i)\n",
    "    w0 = -G.node[sink]['prior_prob'] * G.node[sink]['detect_prob']\n",
    "    for j in range(grand_total_supply):\n",
    "        G.add_edge(sink, 'gt', key = j+1, flow = 0, weight = w0, capacity = 1)\n",
    "        w0 = w0 * (1 - G.node[sink]['detect_prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if IS_DEBUG_MODE:\n",
    "    nodes_pos_dict = {}\n",
    "    for i in range(num_of_source):\n",
    "        nodes_pos_dict['s'+str(i)] = (0, -(i - (num_of_source-1) / 2))\n",
    "    for i in range(num_of_sink):\n",
    "        nodes_pos_dict['t'+str(i)] = (num_of_sink / 2, -(i - (num_of_sink-1) / 2))\n",
    "    nodes_pos_dict['gt'] = (num_of_sink, 0)\n",
    "    # Setting: figure dimensions\n",
    "    plt.rcParams['figure.figsize'] = (10, 6)\n",
    "    nx.draw_networkx(G, pos = nodes_pos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) List all nodes and arcs with properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if IS_DEBUG_MODE:\n",
    "    print(grand_total_supply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if IS_DEBUG_MODE:\n",
    "    for node in G.nodes(data=True):\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if IS_DEBUG_MODE:\n",
    "    for edge in G.edges(data=True):\n",
    "        print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 6 8\n",
      "3\n",
      "10\n",
      "2\n",
      "0.09489644666221526 0.037780526991566185\n",
      "0.18088286777558027 0.8290498018414723\n",
      "0.16444726107147423 0.4341117265469523\n",
      "0.23674540360309848 0.7570344808087831\n",
      "0.28580120790127983 0.01206393228408848\n",
      "0.03722681298635199 0.4464794501737054\n",
      "0 0\n",
      "0 2\n",
      "0 3\n",
      "1 0\n",
      "1 1\n",
      "2 3\n",
      "1 4\n",
      "0 5\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(map(str, [num_of_source, num_of_sink, num_of_arc])))\n",
    "for i in range(num_of_source):\n",
    "    print(G.node['s'+str(i)]['total_supply'])\n",
    "for i in range(num_of_sink):\n",
    "    print(G.node['t'+str(i)]['prior_prob'], end=' ')\n",
    "    print(G.node['t'+str(i)]['detect_prob'])\n",
    "for row in connectivity_info:\n",
    "    print(' '.join(map(str,row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subroutine for finding an augmenting path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If can find an augmenting path, return an empty set; Else, return the isolated group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assign_extra_demand(nodeName):\n",
    "    # Initialization\n",
    "    visited_set = set() # Record visited nodes\n",
    "    Q = queue.Queue()\n",
    "    Q.put(nodeName)\n",
    "    in_Q_set = set() # Record nodes currently in Q (since Python has no queue.contains())\n",
    "    in_Q_set.add(nodeName)\n",
    "    pred = {} # Record predecessors for building augmenting path\n",
    "    for i in range(num_of_source):\n",
    "        pred['s'+str(i)] = None\n",
    "    for i in range(num_of_sink):\n",
    "        pred['t'+str(i)] = None\n",
    "    # Loop\n",
    "    while Q.empty() is False:\n",
    "        nd = Q.get()\n",
    "        in_Q_set.remove(nd)\n",
    "        visited_set.add(nd)\n",
    "        # If the node is a sink\n",
    "        if nd[0] == 't':\n",
    "            for source in G.predecessors(nd):\n",
    "                if source not in in_Q_set and source not in visited_set and source not in elim_set:\n",
    "                    Q.put(source)\n",
    "                    in_Q_set.add(source)\n",
    "                    pred[source] = nd\n",
    "        # If the node is a source, but it has no unused supply\n",
    "        elif nd[0] == 's' and G.node[nd]['unused_supply'] == 0:\n",
    "            for sink in G.neighbors(nd):\n",
    "                if G.edge[nd][sink][0]['flow'] > 0 and sink not in in_Q_set and sink not in visited_set and sink not in elim_set:\n",
    "                    Q.put(sink)\n",
    "                    in_Q_set.add(sink)\n",
    "                    pred[sink] = nd\n",
    "        # If the node is a source, and it has unused supply\n",
    "        else: #(nd[0] == 's' and G.node[nd]['unused_supply'] > 0):\n",
    "            # Decrement unused_supply of the source by 1\n",
    "            G.node[nd]['unused_supply'] -= 1 \n",
    "            # Recursively build augmenting path\n",
    "            cur = nd\n",
    "            while True:\n",
    "                pre = pred[cur]\n",
    "                if pre == None:\n",
    "                    break\n",
    "                # For source\n",
    "                if cur[0] == 's':\n",
    "                    #old_flow = G.edge[cur][pre][0]['flow']\n",
    "                    #G.add_edge(cur, pre, key = 0, flow = old_flow + 1)\n",
    "                    G.edge[cur][pre][0]['flow'] += 1\n",
    "                # For sink\n",
    "                else:\n",
    "                    #old_flow = G.edge[pre][cur][0]['flow']\n",
    "                    #G.add_edge(pre, cur, key = 0, flow = old_flow - 1)\n",
    "                    G.edge[pre][cur][0]['flow'] -= 1\n",
    "                cur = pre\n",
    "            # There is no node to be eliminated! And remember to break the outer loop!\n",
    "            visited_set = set()\n",
    "            break\n",
    "    return visited_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the timer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time_our_algo = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the algorithm:  \n",
    "\n",
    "- Build a min heap ('heap_of_weights') and put initial weights into it;\n",
    "\n",
    "- Use a set ('elim_set') to track which nodes have been eliminated already;\n",
    "\n",
    "- Record the final demand at each sink in 'final_sink_demand'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heap_of_weights = []\n",
    "for i in range(num_of_sink):\n",
    "    # Each element in heap_of_weights is of the form: {weight, which_sink, search_order(key)}\n",
    "    heapq.heappush(heap_of_weights, (G.edge['t'+str(i)]['gt'][1]['weight'], 't'+str(i), 1))\n",
    "elim_set = set()\n",
    "final_sink_demand = {}\n",
    "for i in range(num_of_sink):\n",
    "    final_sink_demand['t'+str(i)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main body of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while grand_total_supply > 0 and len(elim_set) != num_of_source + num_of_sink:\n",
    "    top_element_heap = heapq.heappop(heap_of_weights)\n",
    "    sink = top_element_heap[1]\n",
    "    if sink in elim_set:\n",
    "        continue\n",
    "    potential_elim_set = assign_extra_demand(sink)\n",
    "    if len(potential_elim_set) == 0:\n",
    "        #if IS_DEBUG_MODE:\n",
    "        #    print(sink, \"{0:.4f}\".format(-top_element_heap[0]))\n",
    "        # Reduce grand total supply from all sources by one\n",
    "        grand_total_supply -= 1\n",
    "        # Increase the final demand of the sink by one\n",
    "        final_sink_demand[sink] += 1\n",
    "        # Set y_{kj} = 1\n",
    "        old_order = top_element_heap[2]\n",
    "        old_weight = top_element_heap[0]\n",
    "        #G.add_edge(sink, 'gt', key = old_order, flow = 1, weight = old_weight, capacity = 1)\n",
    "        G.edge[sink]['gt'][old_order]['flow'] = 1\n",
    "        # Compute the next weight, and add the edge to the graph\n",
    "        new_weight = old_weight * (1 - G.node[sink]['detect_prob'])\n",
    "        new_order = old_order + 1\n",
    "        G.add_edge(sink, 'gt', key = new_order, flow = 0, weight = new_weight, capacity = 1)\n",
    "        # Put the weight into the heap\n",
    "        heapq.heappush(heap_of_weights, (new_weight, sink, new_order))\n",
    "    else:\n",
    "        #if IS_DEBUG_MODE:\n",
    "        #    print(sink, \"{0:.4f}\".format(-top_element_heap[0]), \"-> Failed and found an isolated group! :\", potential_elim_set)\n",
    "        elim_set = elim_set.union(potential_elim_set)\n",
    "    #if IS_DEBUG_MODE:\n",
    "    #    edge_list = [edge for edge in G.edges(data=True) if edge[0][0] == 's' and edge[2]['flow'] > 0]\n",
    "    #    list.sort(edge_list)\n",
    "    #    print(edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.19276213645935059 seconds ---\n"
     ]
    }
   ],
   "source": [
    "time_elapsed_our_algo = time.time() - start_time_our_algo\n",
    "print(\"--- %s seconds ---\" % time_elapsed_our_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flows from sources to sinks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if IS_DEBUG_MODE:\n",
    "    edge_list = [edge for edge in G.edges(data=True) if edge[0][0] == 's' and edge[2]['flow'] > 0]\n",
    "    list.sort(edge_list)\n",
    "    for edge in edge_list:\n",
    "        print(edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flows from sinks to global sink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edge_list_2 = [edge for edge in G.edges(data=True) if edge[0][0] == 't' and edge[2]['flow'] > 0]\n",
    "if IS_DEBUG_MODE:\n",
    "    for edge in edge_list_2:\n",
    "        print(edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show optimal value found by our algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.561294539223331\n"
     ]
    }
   ],
   "source": [
    "flow_cost_our_algo = 0\n",
    "for edge in edge_list_2:\n",
    "    flow_cost_our_algo += edge[2]['weight']\n",
    "print(flow_cost_our_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demand at each sink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if IS_DEBUG_MODE:\n",
    "    demand_list = [(k,v) for k,v in final_sink_demand.items() if v > 0]\n",
    "    list.sort(demand_list)\n",
    "    print(demand_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supply at each source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if IS_DEBUG_MODE:\n",
    "    supply_list = [node for node in G.nodes(data=True) if node[0][0] == 's']\n",
    "    list.sort(supply_list)\n",
    "    supply_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetworkX's built-in algorithm: capacity_scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the timer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time_capacity_scaling = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flow_cost_capacity_scaling, flow_dict_capacity_scaling = nx.capacity_scaling(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.11660194396972656 seconds ---\n"
     ]
    }
   ],
   "source": [
    "time_elapsed_capacity_scaling = time.time() - start_time_capacity_scaling\n",
    "print(\"--- %s seconds ---\" % time_elapsed_capacity_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show optimal value found by the capacity scaling algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.561294539223331\n"
     ]
    }
   ],
   "source": [
    "print(flow_cost_capacity_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
